The current state of theoretical understanding is summarized by Achlioptas (2009).
 The satisfiability threshold conjecture states that, for each k, there is a sharp satisfiability
 threshold rk, such that as the number of variables n →∞, instances below the threshold are
 satisfiable with probability 1, while those above the threshold are unsatisfiable with proba
bility 1. The conjecture was not quite proved by Friedgut (1999): a sharp threshold exists but
 its location might depend on n even as n →∞. Despite significant progress in asymptotic
 analysis of the threshold location for large k (Achlioptas and Peres, 2004; Achlioptas et al.,
 2007), all that can be proved for k=3is that it lies in the range [3.52,4.51]. Current theory
 suggests that a peak in the run time of a SAT solver is not necessarily related to the satisfia
bility threshold, but instead to a phase transition in the solution distribution and structure of
 SAT instances. Empirical results due to Coarfa et al. (2003) support this view. In fact, al
gorithms such as survey propagation (Parisi and Zecchina, 2002; Maneva et al., 2007) take
 advantage of special properties of random SAT instances near the satisfiability threshold and
 greatly outperform general SAT solvers on such instances.
 The best sources for information on satisfiability, both theoretical and practical, are the
 Handbook of Satisfiability (Biere et al., 2009) and the regular International Conferences on
 Theory and Applications of Satisfiability Testing, known as SAT.
 The idea of building agents with propositional logic can be traced back to the seminal
 paper of McCulloch and Pitts (1943), which initiated the field of neural networks. Con
trary to popular supposition, the paper was concerned with the implementation of a Boolean
 circuit-based agent design in the brain. Circuit-based agents, which perform computation by
 propagating signals in hardware circuits rather than running algorithms in general-purpose
 computers, have received little attention in AI, however. The most notable exception is the
 work of Stan Rosenschein (Rosenschein, 1985; Kaelbling and Rosenschein, 1990), who de
veloped ways to compile circuit-based agents from declarative descriptions of the task envi
ronment. (Rosenschein’s approach is described at some length in the second edition of this
 book.) The work of Rod Brooks (1986, 1989) demonstrates the effectiveness of circuit-based
 designs for controlling robots—a topic we take up in Chapter 25. Brooks (1991) argues
 that circuit-based designs are all that is needed for AI—that representation and reasoning
 are cumbersome, expensive, and unnecessary. In our view, neither approach is sufficient by
 itself. Williams et al. (2003) show how a hybrid agent design not too different from our
 wumpus agent has been used to control NASA spacecraft, planning sequences of actions and
 diagnosing and recovering from faults.
 The general problem of keeping track of a partially observable environment was intro
duced for state-based representations in Chapter 4. Its instantiation for propositional repre
sentations was studied by Amir and Russell (2003), who identified several classes of envi
ronments that admit efficient state-estimation algorithms and showed that for several other
 classes the problem is intractable. The temporal-projection problem, which involves deter
mining what propositions hold true after an action sequence is executed, can be seen as a
 special case of state estimation with empty percepts. Many authors have studied this problem
 because of its importance in planning; some important hardness results were established by
Exercises
 279
 Liberatore (1997). The idea of representing a belief state with propositions can be traced to
 Wittgenstein (1922).
 Logical state estimation, of course, requires a logical representation of the effects of
 actions—a key problem in AI since the late 1950s. The dominant proposal has been the sit
uation calculus formalism (McCarthy, 1963), which is couched within first-order logic. We
 discuss situation calculus, and various extensions and alternatives, in Chapters 10 and 12. The
 approach taken in this chapter—using temporal indices on propositional variables—is more
 restrictive but has the benefit of simplicity. The general approach embodied in the SATPLAN
 algorithm was proposed by Kautz and Selman (1992). Later generations of SATPLAN were
 able to take advantage of the advances in SAT solvers, described earlier, and remain among
 the most effective ways of solving difficult problems (Kautz, 2006).
 Thepropertiesofdepth-first searchdependstronglyonwhether thegraph-searchor
 tree-searchversionisused. Thegraph-searchversion,whichavoidsrepeatedstatesandre
dundantpaths,iscompleteinfinitestatespacesbecauseitwilleventuallyexpandeverynode.
 Thetree-searchversion,ontheotherhand, isnotcomplete—forexample, inFigure3.6the
 algorithmwillfollowtheArad–Sibiu–Arad–Sibiuloopforever.Depth-firsttreesearchcanbe
 modifiedatnoextramemorycostsothatitchecksnewstatesagainst thoseonthepathfrom
 theroottothecurrentnode;thisavoidsinfiniteloopsinfinitestatespacesbutdoesnotavoid
 theproliferationofredundantpaths. Ininfinitestatespaces,bothversionsfail ifaninfinite
 non-goalpathisencountered. Forexample, inKnuth’s4problem,depth-firstsearchwould
 keepapplyingthefactorialoperatorforever.
 Forsimilarreasons,bothversionsarenonoptimal.Forexample, inFigure3.16,depth
firstsearchwillexploretheentireleftsubtreeevenifnodeCisagoalnode. IfnodeJwere
 alsoagoalnode, thendepth-first searchwouldreturnitasasolutioninsteadofC,which
 wouldbeabettersolution;hence,depth-firstsearchisnotoptimal.
Section 3.4.
 Uninformed Search Strategies
 87
 The time complexity of depth-first graph search is bounded by the size of the state space
 (which may be infinite, of course). A depth-first tree search, on the other hand, may generate
 alloftheO(bm) nodes in the search tree, where m is the maximum depth of any node; this
 can be much greater than the size of the state space. Note that m itself can be much larger
 than d (the depth of the shallowest solution) and is infinite if the tree is unbounded.
 So far, depth-first search seems to have no clear advantage over breadth-first search,
 so why do we include it? The reason is the space complexity. For a graph search, there is
 no advantage, but a depth-first tree search needs to store only a single path from the root
 to a leaf node, along with the remaining unexpanded sibling nodes for each node on the
 path. Once a node has been expanded, it can be removed from memory as soon as all its
 descendants have been fully explored. (See Figure 3.16.) For a state space with branching
 factor b and maximum depth m, depth-first search requires storage of only O(bm) nodes.
 Using the same assumptions as for Figure 3.13 and assuming that nodes at the same depth as
 the goal node have no successors, we find that depth-first search would require 156 kilobytes
 instead of 10 exabytes at depth d =16, a factor of 7 trillion times less space. This has
 led to the adoption of depth-first tree search as the basic workhorse of many areas of AI,
 including constraint satisfaction (Chapter 6), propositional satisfiability (Chapter 7), and logic
 programming (Chapter 9). For the remainder of this section, we focus primarily on the tree
search version of depth-first search.
 BACKTRACKING
 SEARCH
 DEPTH-LIMITED
 SEARCH
 Avariant of depth-first search called backtracking search uses still less memory. (See
 Chapter 6 for more details.) In backtracking, only one successor is generated at a time rather
 than all successors; each partially expanded node remembers which successor to generate
 next. In this way, only O(m) memory is needed rather than O(bm). Backtracking search
 facilitates yet another memory-saving (and time-saving) trick: the idea of generating a suc
cessor by modifying the current state description directly rather than copying it first. This
 reduces the memory requirements to just one state description and O(m) actions. For this to
 work, we must be able to undo each modification when we go back to generate the next suc
cessor. For problems with large state descriptions, such as robotic assembly, these techniques
 are critical to success.
 3.4.4 Depth-limited search
 The embarrassing failure of depth-first search in infinite state spaces can be alleviated by
 supplying depth-first search with a predetermined depth limit . That is, nodes at depth are
 treated as if they have no successors. This approach is called depth-limited search. The
 depth limit solves the infinite-path problem. Unfortunately, it also introduces an additional
 source of incompleteness if we choose <d, that is, the shallowest goal is beyond the depth
 limit. (This is likely when d is unknown.) Depth-limited search will also be nonoptimal if
 we choose >d. Its time complexity is O(b) and its space complexity is O(b ). Depth-first
 search can be viewed as a special case of depth-limited search with =∞.
 Sometimes, depth limits can be based on knowledge of the problem. For example, on
 the map of Romania there are 20 cities. Therefore, we know that if there is a solution, it must
 be of length 19 at the longest, so =19is a possible choice. But in fact if we studied the
Iterative deepening search (or iterative deepening depth-first search) is a general strategy,
 often used in combination with depth-first tree search, that finds the best depth limit. It does
 this by gradually increasing the limit—first 0, then 1, then 2, and so on—until a goal is found.
 This will occur when the depth limit reaches d, the depth of the shallowest goal node. The
 algorithm is shown in Figure 3.18. Iterative deepening combines the benefits of depth-first
 and breadth-first search. Like depth-first search, its memory requirements are modest: O(bd)
 to be precise. Like breadth-first search, it is complete when the branching factor is finite and
 optimal when the path cost is a nondecreasing function of the depth of the node. Figure 3.19
 shows four iterations of ITERATIVE-DEEPENING-SEARCH on a binary search tree, where the
 solution is found on the fourth iteration.
 Iterative deepening search may seem wasteful because states are generated multiple
 times. It turns out this is not too costly. The reason is that in a search tree with the same (or
 nearly the same) branching factor at each level, most of the nodes are in the bottom level,
 so it does not matter much that the upper levels are generated multiple times. In an iterative
 deepening search, the nodes on the bottom level (depth d) are generated once, those on the
The idea behind bidirectional search is to run two simultaneous searches—one forward from
 the initial state and the other backward from the goal—hoping that the two searches meet in
 the middle (Figure 3.20). The motivation is that bd/2 + bd/2 is much less than bd,orinthe
 f
 igure, the area of the two small circles is less than the area of one big circle centered on the
 start and reaching to the goal.
 Bidirectional search is implemented by replacing the goal test with a check to see
 whether the frontiers of the two searches intersect; if they do, a solution has been found.
 (It is important to realize that the first such solution found may not be optimal, even if the
 two searches are both breadth-first; some additional search is required to make sure there
 isn’t another short-cut across the gap.) The check can be done when each node is generated
 or selected for expansion and, with a hash table, will take constant time. For example, if a
 problem has solution depth d=6, and each direction runs breadth-first search one node at a
 time, then in the worst case the two searches meet when they have generated all of the nodes
 at depth 3. For b=10, this means a total of 2,220 node generations, compared with 1,111,110
 for a standard breadth-first search. Thus, the time complexity of bidirectional search using
 breadth-first searches in both directions is O(bd/2). The space complexity is also O(bd/2).
 Wecan reduce this by roughly half if one of the two searches is done by iterative deepening,
 but at least one of the frontiers must be kept in memory so that the intersection check can be
 done. This space requirement is the most significant weakness of bidirectional search.
Greedy best-first search8 tries to expand the node that is closest to the goal, on the grounds
 that this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the
 heuristic function; that is, f(n)=h(n).
 Let us see how this works for route-finding problems in Romania; we use the straight
line distance heuristic, which we will call hSLD. If the goal is Bucharest, we need to
 know the straight-line distances to Bucharest, which are shown in Figure 3.22. For exam
ple, hSLD(In(Arad))=366. Notice that the values of hSLD cannot be computed from the
 problem description itself. Moreover, it takes a certain amount of experience to know that
 hSLD is correlated with actual road distances and is, therefore, a useful heuristic.
 Figure 3.23 shows the progress of a greedy best-first search using hSLD to find a path
 from Arad to Bucharest. The first node to be expanded from Arad will be Sibiu because it
 is closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will
 be Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. For
 this particular problem, greedy best-first search using hSLD finds a solution without everThe first condition we require for optimality is that h(n) be an admissible heuristic. An
 admissible heuristic is one that never overestimates the cost to reach the goal. Because g(n)
 is the actual cost to reach n along the current path, and f(n)=g(n)+h(n),wehaveasan
 immediate consequence that f(n) never overestimates the true cost of a solution along the
 current path through n.
 Admissible heuristics are by nature optimistic because they think the cost of solving
 the problem is less than it actually is. An obvious example of an admissible heuristic is the
 straight-line distance hSLD that we used in getting to Bucharest. Straight-line distance is
 admissible because the shortest path between any two points is a straight line, so the straight
Section 3.5.
 Informed (Heuristic) Search Strategies
 95
 line cannot be an overestimate. In Figure 3.24, we show the progress of an A∗ tree search for
 Bucharest. The values of g are computed from the step costs in Figure 3.2, and the values of
 hSLD are given in Figure 3.22. Notice in particular that Bucharest first appears on the frontier
 at step (e), but it is not selected for expansion because its f-cost (450) is higher than that of
 Pitesti (417). Another way to say this is that there might be a solution through Pitesti whose
 cost is as low as 417, so the algorithm will not settle for a solution that costs 450.
 CONSISTENCY
 MONOTONICITY
 TRIANGLE
 INEQUALITY
 Asecond, slightly stronger condition called consistency (or sometimes monotonicity)
 is required only for applications of A∗ to graph search.9 A heuristic h(n) is consistent if, for
 every node n and every successor n of n generated by any action a, the estimated cost of
 reaching the goal from n is no greater than the step cost of getting to n plus the estimated
 cost of reaching the goal from n:
 h(n) ≤ c(n,a,n)+h(n) .
 This is a form of the general triangle inequality, which stipulates that each side of a triangle
 cannot be longer than the sum of the other two sides. Here, the triangle is formed by n, n,
 and the goal Gn closest to n. For an admissible heuristic, the inequality makes perfect sense:
 if there were a route from n to Gn via n that was cheaper than h(n), that would violate the
 property that h(n) is a lower bound on the cost to reach Gn.
 It is fairly easy to show (Exercise 3.29) that every consistent heuristic is also admissible.
 Consistency is therefore a stricter requirement than admissibility, but one has to work quite
 hard to concoct heuristics that are admissible but not consistent. All the admissible heuristics
 we discuss in this chapter are also consistent. Consider, for example, hSLD. We know that
 the general triangle inequality is satisfied when each side is measured by the straight-line
 distance and that the straight-line distance between n and n is no greater than c(n,a,n).
 Hence, hSLD is a consistent heuristic.
 Optimality of A*
 As we mentioned earlier, A∗ has the following properties: the tree-search version of A∗ is
 optimal if h(n) is admissible, while the graph-search version is optimal if h(n) is consistent.
 We show the second of these two claims since it is more useful. The argument es
sentially mirrors the argument for the optimality of uniform-cost search, with g replaced by
 f—just as in the A∗ algorithm itself.
 The first step is to establish the following: if h(n) is consistent, then the values of
 f(n) along any path are nondecreasing. The proof follows directly from the definition of
 consistency. Suppose n is a successor of n; theng(n)=g(n)+c(n,a,n) for some action
 a, and we have
 f(n)=g(n)+h(n)=g(n)+c(n,a,n)+h(n) ≥ g(n)+h(n)=f(n).
 The next step is to prove that whenever A∗ selects a node n for expansion, the optimal path
 to that node has been found. Were this not the case, there would have to be another frontier
 node n on the optimal path from the start node to n, by the graph separation property of Figure 3.9; because f is nondecreasing along any path, nwould have lower f-cost than n
 and would have been selected first.
 From the two preceding observations, it follows that the sequence of nodes expanded
 by A∗ using GRAPH-SEARCH is in nondecreasing order of f(n). Hence, the first goal node
 selected for expansion must be an optimal solution because f is the true cost for goal nodes
 (which have h=0) and all later goal nodes will be at least as expensive.
 CONTOUR
 The fact that f-costs are nondecreasing along any path also means that we can draw
 contours in the state space, just like the contours in a topographic map. Figure 3.25 shows
 an example. Inside the contour labeled 400, all nodes have f(n) less than or equal to 400,
 and so on. Then, because A∗ expands the frontier node of lowest f-cost, we can see that an
 A∗ search fans out from the start node, adding nodes in concentric bands of increasing f-cost.
 With uniform-cost search (A∗ search using h(n)=0), the bands will be “circular”
 around the start state. With more accurate heuristics, the bands will stretch toward the goal
 state and become more narrowly focused around the optimal path. If C∗ is the cost of the
 optimal solution path, then we can say the following:
 • A∗ expands all nodes with f(n) <C∗.
 • A∗mightthen expand some ofthe nodes right on the “goal contour” (where f(n)=C∗)
 before selecting a goal node.
 Completeness requires that there be only finitely many nodes with cost less than or equal to
 C∗, a condition that is true if all step costs exceed some finite and if b is finite.
 Notice that A∗ expands no nodes with f(n) >C∗—for example, Timisoara is not
 expanded in Figure 3.24 even though it is a child of the root. We say that the subtree below
98
 Chapter 3.
 Solving Problems by Searching
 PRUNING
 OPTIMALLY
 EFFICIENT
 ABSOLUTE ERROR
 RELATIVE ERROR
 Timisoara is pruned; because hSLD is admissible, the algorithm can safely ignore this subtree
 while still guaranteeing optimality. The concept of pruning—eliminating possibilities from
 consideration without having to examine them—is important for many areas of AI.
 One final observation is that among optimal algorithms of this type—algorithms that
 extend search paths from the root and use the same heuristic information—A∗ is optimally
 efficient for any given consistent heuristic. That is, no other optimal algorithm is guaran
teed to expand fewer nodes than A∗ (except possibly through tie-breaking among nodes with
 f(n)=C∗). This is because any algorithm that does not expand all nodes with f(n) <C∗
 runs the risk of missing the optimal solution.
 That A∗ search is complete, optimal, and optimally efficient among all such algorithms
 is rather satisfying. Unfortunately, it does not mean that A∗ is the answer to all our searching
 needs. The catch is that, for most problems, the number of states within the goal contour
 search space is still exponential in the length of the solution. The details of the analysis are
 beyond the scope of this book, but the basic results are as follows. For problems with constant
 step costs, the growth in run time as a function of the optimal solution depth d is analyzed in
 terms of the the absolute error or the relative error of the heuristic. The absolute error is
 defined as Δ ≡ h∗ −h,whereh∗ is the actual cost of getting from the root to the goal, and
 the relative error is defined as ≡ (h∗ − h)/h∗.
 The complexity results depend very strongly on the assumptions made about the state
 space. The simplest model studied is a state space that has a single goal and is essentially a
 tree with reversible actions. (The 8-puzzle satisfies the first and third of these assumptions.)
 In this case, the time complexity of A∗ is exponential in the maximum absolute error, that is,
 O(bΔ). For constant step costs, we can write this as O(b d),whered is the solution depth.
 For almost all heuristics in practical use, the absolute error is at least proportional to the path
 cost h∗, so is constant or growing and the time complexity is exponential in d. We can
 also see the effect of a more accurate heuristic: O(b d)=O((b)d), so the effective branching
 factor (defined more formally in the next section) is b .
 When the state space has many goal states—particularly near-optimal goal states—the
 search process can be led astray from the optimal path and there is an extra cost proportional
 to the number of goals whose cost is within a factor of the optimal cost. Finally, in the
 general case of a graph, the situation is even worse. There can be exponentially many states
 with f(n) <C∗ even if the absolute error is bounded by a constant. For example, consider
 a version of the vacuum world where the agent can clean up any square for unit cost without
 even having to visit it: in that case, squares can be cleaned in any order. With N initially dirty
 squares, there are 2N states where some subset has been cleaned and all of them are on an
 optimal solution path—and hence satisfy f(n) <C∗—even if the heuristic has an error of 1.
 The complexity of A∗ often makes it impractical to insist on finding an optimal solution.
 One can use variants of A∗ that find suboptimal solutions quickly, or one can sometimes
 design heuristics that are more accurate but not strictly admissible. In any case, the use of a
 good heuristic still provides enormous savings compared to the use of an uninformed search.
 In Section 3.6, we look at the question of designing good heuristics.
 Computation time is not, however, A∗’s main drawback. Because it keeps all generated
 nodes in memory (as do all GRAPH-SEARCH algorithms), A∗ usually runs out of space long
 before it runs out of time. For this reason, A∗ is not practical for many large-scale prob
lems. There are, however, algorithms that overcome the space problem without sacrificing
 optimality or completeness, at a small cost in execution time. We discuss these next.
 The simplest way to reduce memory requirements for A∗ is to adapt the idea of iterative
 deepening to the heuristic search context, resulting in the iterative-deepening A∗ (IDA∗)al
gorithm. The main difference between IDA∗ and standard iterative deepening is that the cutoff
 used is the f-cost (g+h) rather than the depth; at each iteration, the cutoff value is the small
est f-cost of any node that exceeded the cutoff on the previous iteration. IDA∗ is practical
 for many problems with unit step costs and avoids the substantial overhead associated with
 keeping a sorted queue of nodes. Unfortunately, it suffers from the same difficulties with real
valued costs as does the iterative version of uniform-cost search described in Exercise 3.17.
 This section briefly examines two other memory-bounded algorithms, called RBFS and MA∗.
 Recursive best-first search (RBFS) is a simple recursive algorithm that attempts to
 mimic the operation of standard best-first search, but using only linear space. The algorithm
 is shown in Figure 3.26. Its structure is similar to that of a recursive depth-first search, but
 rather than continuing indefinitely down the current path, it uses the f limit variable to keep
 track of the f-value of the best alternative path available from any ancestor of the current
 node. If the current node exceeds this limit, the recursion unwinds back to the alternative
 path. As the recursion unwinds, RBFS replaces the f-value of each node along the path
 with a backed-up value—the best f-value of its children. In this way, RBFS remembers the
 f-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth
changes its mind” and tries Fagaras, and then changes its mind back again. These mind
 changes occur because every time the current best path is extended, its f-value is likely to
 increase—h is usually less optimistic for nodes closer to the goal. When this happens, the
 second-best path might become the best path, so the search has to backtrack to follow it.
 Each mind change corresponds to an iteration of IDA∗ and could require many reexpansions
 of forgotten nodes to recreate the best path and extend it one more node.
 Like A∗ tree search, RBFS is an optimal algorithm if the heuristic function h(n) is
 admissible. Its space complexity is linear in the depth of the deepest optimal solution, but
 its time complexity is rather difficult to characterize: it depends both on the accuracy of the
 heuristic function and on how often the best path changes as nodes are expanded.
 IDA∗ and RBFS suffer from using too little memory. Between iterations, IDA∗ retains
 only a single number: the current f-cost limit. RBFS retains more information in memory,
 but it uses only linear space: even if more memory were available, RBFS has no way to make
 use of it. Because they forget most of what they have done, both algorithms may end up reex
panding the same states many times over. Furthermore, they suffer the potentially exponential
 increase in complexity associated with redundant paths in graphs (see Section 3.3).
 MA*
 SMA*
 It seems sensible, therefore, to use all available memory. Two algorithms that do this
 are MA∗ (memory-bounded A∗)andSMA∗ (simplified MA∗). SMA∗ is—well—simpler, so
 we will describe it. SMA∗ proceeds just like A∗, expanding the best leaf until memory is full.
 At this point, it cannot add a new node to the search tree without dropping an old one. SMA∗
 always drops the worst leaf node—the one with the highest f-value. Like RBFS, SMA∗
 then backs up the value of the forgotten node to its parent. In this way, the ancestor of a
 forgotten subtree knows the quality of the best path in that subtree. With this information,
 SMA∗ regenerates the subtree only when all other paths have been shown to look worse than
 the path it has forgotten. Another way of saying this is that, if all the descendants of a node n
 are forgotten, then we will not know which way to go from n, but we will still have an idea
 of how worthwhile it is to go anywhere from n.
 The complete algorithm is too complicated to reproduce here,10 but there is one subtlety
 worth mentioning. We said that SMA∗ expands the best leaf and deletes the worst leaf. What
 if all the leaf nodes have the same f-value? To avoid selecting the same node for deletion
 and expansion, SMA∗ expands the newest best leaf and deletes the oldest worst leaf. These
 coincide when there is only one leaf, but in that case, the current search tree must be a single
 path from root to leaf that fills all of memory. If the leaf is not a goal node, then even if it is on
 an optimal solution path, that solution is not reachable with the available memory. Therefore,
 the node can be discarded exactly as if it had no successors.
 SMA∗ is complete if there is any reachable solution—that is, if d, the depth of the
 shallowest goal node, is less than the memory size (expressed in nodes). It is optimal if any
 optimal solution is reachable; otherwise, it returns the best reachable solution. In practical
 terms, SMA∗ is a fairly robust choice for finding optimal solutions, particularly when the state
 space is a graph, step costs are not uniform, and node generation is expensive compared to
 the overhead of maintaining the frontier and the explored set.
 10 Arough sketch appeared in the first edition of this book.
102
 Chapter 3.
 Solving Problems by Searching
 THRASHING
 METALEVEL STATE
 SPACE
 OBJECT-LEVEL STATE
 SPACE
 METALEVEL
 LEARNING
 Onvery hard problems, however, it will often be the case that SMA∗ is forced to switch
 back and forth continually among many candidate solution paths, only a small subset of which
 can fit in memory. (This resembles the problem of thrashing in disk paging systems.) Then
 the extra time required for repeated regeneration of the same nodes means that problems
 that would be practically solvable by A∗, given unlimited memory, become intractable for
 SMA∗. That is to say, memory limitations can make a problem intractable from the point
 of view of computation time. Although no current theory explains the tradeoff between time
 and memory, it seems that this is an inescapable problem. The only way out is to drop the
 optimality requirement.
 3.5.4 Learning to search better
 We have presented several fixed strategies—breadth-first, greedy best-first, and so on—that
 have been designed by computer scientists. Could an agent learn how to search better? The
 answer is yes, and the method rests on an important concept called the metalevel state space.
 Each state in a metalevel state space captures the internal (computational) state of a program
 that is searching in an object-level state space such as Romania. For example, the internal
 state of the A∗ algorithm consists of the current search tree. Each action in the metalevel state
 space is a computation step that alters the internal state; for example, each computation step
 in A∗ expands a leaf node and adds its successors to the tree. Thus, Figure 3.24, which shows
 a sequence of larger and larger search trees, can be seen as depicting a path in the metalevel
 state space where each state on the path is an object-level search tree.
 Now,the path in Figure 3.24 has five steps, including one step, the expansion of Fagaras,
 that is not especially helpful. For harder problems, there will be many such missteps, and a
 metalevel learning algorithm can learn from these experiences to avoid exploring unpromis
ing subtrees. The techniques used for this kind of learning are described in Chapter 21. The
 goal of learning is to minimize the total cost of problem solving, trading off computational
 expense and path cost.
 3.6 HEURISTIC FUNCTIONS
 In this section, we look at heuristics for the 8-puzzle, in order to shed light on the nature of
 heuristics in general.
 The 8-puzzle was one of the earliest heuristic search problems. As mentioned in Sec
tion 3.2, the object of the puzzle is to slide the tiles horizontally or vertically into the empty
 space until the configuration matches the goal configuration (Figure 3.28).
 The average solution cost for a randomly generated 8-puzzle instance is about 22 steps.
 The branching factor is about 3. (When the empty tile is in the middle, four moves are
 possible; when it is in a corner, two; and when it is along an edge, three.) This means
 that an exhaustive tree search to depth 22 would look at about 322 ≈ 3.1×1010 states.
 A graph search would cut this down by a factor of about 170,000 because only 9!/2=
 181,440 distinct states are reachable. (See Exercise 3.4.) This is a manageable number, but
For other uses, see Heuristic (disambiguation).
Complex systems
Topics
Self-organization
Collective behavior
Networks
Evolution and adaptation
Pattern formation
Systems theory and cybernetics
Nonlinear dynamics
Game theory
vte
Not to be confused with Eureka (word).
A heuristic[1] or heuristic technique (problem solving, mental shortcut, rule of thumb)[2][3][4][5] is any approach to problem solving that employs a pragmatic method that is not fully optimized, perfected, or rationalized, but is nevertheless "good enough" as an approximation or attribute substitution.[6][7] Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution.[8][9] Heuristics can be mental shortcuts that ease the cognitive load of making a decision.[10][11][12]

Heuristic reasoning is often based on induction, or on analogy ... Induction is the process of discovering general laws  ... Induction tries to find regularity and coherence ... Its most conspicuous instruments are generalization, specialization, analogy. [...] Heuristic discusses human behavior in the face of problems [... that have been] preserved in the wisdom of proverbs.[13]

— George Pólya, How to Solve It
Context
Main article: Strategy
Part of a series on
Strategy
Strategy topics
Analysis methods
Strategy • Strategic management
Military strategy • Strategic studies
Strategic planning • Strategic thinking
Decision theory • Game theory
Major thinkers
Michael Porter  • Rita Gunther McGrath
Bruce Henderson  • Gary Hamel
Candace A. Yano  • C. K. Prahalad
Jim Collins  • Liddell Hart
Carl von Clausewitz  • Sun Tzu
Julian Corbett  • Alfred Thayer Mahan
J.C. Wylie  • Adrian Slywotzky
Sharon Oster  • Chris Zook
Henry Mintzberg
Concepts
Business model  • Competitive advantage
Value chain  • Performance effects
Core competency • Generic strategies
Mission statement
Frameworks and tools
SWOT • Five forces
Balanced scorecard • Ansoff matrix
OGSM • Managerial grid model
PEST analysis • Growth–share matrix
STP • MECE principle
Business Model Canvas • Kraljic matrix
Strategic Grid Model • Strategy map • VRIO
vte
For broader coverage of this topic, see Trial and error, Rule of thumb, and Guessing.
Gigerenzer & Gaissmaier (2011) state that sub-sets of strategy include heuristics, regression analysis, and Bayesian inference.[14]

A heuristic is a strategy that ignores part of the information, with the goal of making decisions more quickly, frugally, and/or accurately than more complex methods (Gigerenzer and Gaissmaier [2011], p. 454; see also Todd et al. [2012], p. 7).[15]

— S. Chow, "Many Meanings of 'Heuristic'", The British Journal for the Philosophy of Science
Heuristics are strategies based on rules to generate optimal decisions, like the anchoring effect and utility maximization problem.[16] These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues.[17][18] When an individual applies a heuristic in practice, it generally performs as expected. However it can alternatively create systematic errors.[19]

The most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems. In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification.

Dual process theory concerns embodied heuristics.[20]

In psychology, heuristics are simple, efficient rules, either learned or inculcated by evolutionary processes. These psychological heuristics have been proposed to explain how people make decisions, come to judgements, and solve problems. These rules typically come into play when people face complex problems or incomplete information. Researchers employ various methods to test whether people use these rules. The rules have been shown to work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases.[21]

Heuristic rigour models
Part of a series on
Epistemology
OutlineCategoryIndex
Schools
Concepts
Domains
Epistemologists
Related fields
vte
detail of a Tree of Knowledge after Diderot & d'Alembert's Encyclopédie, by Chrétien Frédéric Guillaume Roth
Information mapping
Topics and fields
Business decision mappingData visualizationGraphic communicationInfographicsInformation designKnowledge visualizationMental modelMorphological analysisOntology (information science)Schema (psychology)Visual analyticsVisual language
Node–link approaches
Argument mapCladisticsCognitive mapConcept latticeConcept mapConceptual graphDecision treeDendrogramGraph drawingHyperbolic treeHypertextIssue mapIssue treeLayered graph drawingMind mapObject-role modelingOrganizational chartPathfinder networkRadial treeSemantic networkSociogramTimelineTopic mapTree structureZigZag
See also
Design rationaleDiagrammatic reasoningEntity–relationship modelGeovisualizationList of concept- and mind-mapping softwareOlogOntology (philosophy)Problem structuring methodsSemantic WebTreemappingWicked problem
vte
	
This list is incomplete; you can help by adding missing items. (May 2024)
Main article: Rigour
For broader coverage of this topic, see Expectation (epistemic) and Cybernetics.
See also: Heuristic (computer science) and Heuristic evaluation
Lakatosian heuristics is based on the key term: Justification (epistemology).[22]

One-reason decisions
See also: Optimal stopping
One-reason decisions are algorithms that are made of three rules: search rules, confirmation rules (stopping), and decision rules[23][24][25]

Take-the-best heuristic – Decision-making strategy[26][27][28]
Hiatus heuristic: a "recency-of-last-purchase rule"[29]
Default effect – Tendency to accept the default option[30]
Priority heuristic[31]
Take-the-first heuristic[32]
Recognition-based decisions
A class that's function is to determine and filter out superfluous things.[33]

Recognition heuristic – Decision-making Concept in Psychology[34][35]
Fluency heuristic[36][37]
Tracking heuristics
For broader coverage of this topic, see Predation.
Tracking heuristics is a class of heuristics.[38]

Gaze heuristic[38]
Pointing and calling – Railway safety technique
Trade-off
Trade-off – Situational decision[39]
Tallying heuristic[40][41]
Equality heuristic[42]
Social heuristics
Social heuristics – Decision-making processes in social environments[43]

Imitation – Behaviour in which an individual observes and replicates another's behaviour[44]
Tit for tat – English saying meaning "equivalent retaliation"[45]
Wisdom of the crowd – Collective perception of a group of people[46]
Epistemic heuristics
For broader coverage of this topic, see Tacit assumption.
Propositional attitude – Concept in epistemology[47]
Essence – That which makes or defines an entity what it is[48]
Analysis – Process of understanding a complex topic or substance[49]
Falsifiability – Property of a statement that can be logically contradicted[50]
Hierarchy of evidence – Heuristic ranking science research results
Behavioral economics
Main article: Behavioral economics
Affect heuristic – Mental shortcut based on emotion[51]
Feedback – Process where information about current status is used to influence future status[52]
Reinforcement – Consequence affecting an organism's future behavior[52]
Stimulus–response model – Conceptual framework in psychology[52]
Others
Satisficing – Cognitive heuristic of searching for an acceptable decision[53][54][55]
Representativeness heuristic – Tool for assisting judgement in uncertainty[56][57][58]
Availability heuristic – Bias towards recently acquired information[59][60]
Awareness – Perception or knowledge of something[61]
Base and superstructure – Model of society in Marxist theory[62]
Social organism – Model of social interactions[62]
Dialectic – Discursive method of arriving at the truth by way of reasoned contradiction and argumentation[62]
Continuum limit – Continuum limit in lattice models[63]
Johari window – Technique in personality development
Social rationality
Desert (philosophy) – Condition of being deserving of something, whether good or bad[64]
Less-is-better effect – Cognitive bias
Minimalist heuristic[65]
Unification of theories in physics – Idea of connecting all of physics into one set of equations[66]
Backward induction – Process of reasoning backwards in sequence
Meta-heuristic
Main article: Metaheuristic
For broader coverage of this topic, see List of metaphor-based metaheuristics.
Optimality[67]
Survival of the fittest – Phrase to describe the mechanism of natural selection[67]
Mechanical equilibrium – When the net force on a particle is zero[67]
Chemical equilibrium – When the ratio of reactants to products of a chemical reaction is constant with time[67]
Homeostasis – State of steady internal conditions maintained by living things[67]
Entropy – Property of a thermodynamic system[67]
History
Main article: Analysis
For broader coverage of this topic, see Dialectic.
George Polya studied and published on heuristics in 1945.[68] Polya (1945) cites Pappus of Alexandria as having written a text that Polya dubs Heuristic.[69] Pappus' heuristic problem-solving methods consist of analysis and synthesis.[70]
Modern
Johannes Kepler's work Stereometria Doliorum (1615) formed the basis of integral calculus.[17] Kepler developed a method to calculate the area of an ellipse by adding up the lengths of many radii drawn from a focus of the ellipse.[18]

Significant work was a treatise, the origin being Kepler's methods,[18] written by Bonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes' in The Method, but this treatise is believed to have been lost in the 13th century and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri's work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.

The formal study of calculus brought together Cavalieri's infinitesimals with the calculus of finite differences developed in Europe at around the same time. Pierre de Fermat, claiming that he borrowed from Diophantus, introduced the concept of adequality, which represented equality up to an infinitesimal error term.[19] The combination was achieved by John Wallis, Isaac Barrow, and James Gregory, the latter two proving predecessors to the second fundamental theorem of calculus around 1670.[20][21]

The product rule and chain rule,[22] the notions of higher derivatives and Taylor series,[23] and of analytic functions[24] were used by Isaac Newton in an idiosyncratic notation which he applied to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his Principia Mathematica (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.[25]


Gottfried Wilhelm Leibniz was the first to state clearly the rules of calculus.

Isaac Newton developed the use of calculus in his laws of motion and universal gravitation.
These ideas were arranged into a true calculus of infinitesimals by Gottfried Wilhelm Leibniz, who was originally accused of plagiarism by Newton.[26] He is now regarded as an independent inventor of and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing the product rule and chain rule, in their differential and integral forms. Unlike Newton, Leibniz put painstaking effort into his choices of notation.[27]

Today, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to general physics. Leibniz developed much of the notation used in calculus today.[28]: 51–52  The basic insights that both Newton and Leibniz provided were the laws of differentiation and integration, emphasizing that differentiation and integration are inverse processes, second and higher derivatives, and the notion of an approximating polynomial series.

When Newton and Leibniz first published their results, there was great controversy over which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in his Method of Fluxions), but Leibniz published his "Nova Methodus pro Maximis et Minimis" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of the Royal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics.[29] A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation. It is Leibniz, however, who gave the new discipline its name. Newton called his calculus "the science of fluxions", a term that endured in English schools into the 19th century.[30]: 100  The first complete treatise on calculus to be written in English and use the Leibniz notation was not published until 1815.[31]


Maria Gaetana Agnesi
Since the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi.[32][33]

Foundations
In calculus, foundations refers to the rigorous development of the subject from axioms and definitions. In early calculus, the use of infinitesimal quantities was thought unrigorous and was fiercely criticized by several authors, most notably Michel Rolle and Bishop Berkeley. Berkeley famously described infinitesimals as the ghosts of departed quantities in his book The Analyst in 1734. Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz, and is still to some extent an active area of research today.[34]

Several mathematicians, including Maclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work of Cauchy and Weierstrass, a way was finally found to avoid mere "notions" of infinitely small quantities.[35] The foundations of differential and integral calculus had been laid. In Cauchy's Cours d'Analyse, we find a broad range of foundational approaches, including a definition of continuity in terms of infinitesimals, and a (somewhat imprecise) prototype of an (ε, δ)-definition of limit in the definition of differentiation.[36] In his work, Weierstrass formalized the concept of limit and eliminated infinitesimals (although his definition can validate nilsquare infinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called "infinitesimal calculus". Bernhard Riemann used these ideas to give a precise definition of the integral.[37] It was also during this period that the ideas of calculus were generalized to the complex plane with the development of complex analysis.[38]

In modern mathematics, the foundations of calculus are included in the field of real analysis, which contains full definitions and proofs of the theorems of calculus. The reach of calculus has also been greatly extended. Henri Lebesgue invented measure theory, based on earlier developments by Émile Borel, and used it to define integrals of all but the most pathological functions.[39] Laurent Schwartz introduced distributions, which can be used to take the derivative of any function whatsoever.[40]

Limits are not the only rigorous approach to the foundation of calculus. Another way is to use Abraham Robinson's non-standard analysis. Robinson's approach, developed in the 1960s, uses technical machinery from mathematical logic to augment the real number system with infinitesimal and infinite numbers, as in the original Newton-Leibniz conception. The resulting numbers are called hyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus.[41] There is also smooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher-power infinitesimals during derivations.[34] Based on the ideas of F. W. Lawvere and employing the methods of category theory, smooth infinitesimal analysis views all functions as being continuous and incapable of being expressed in terms of discrete entities. One aspect of this formulation is that the law of excluded middle does not hold.[34] The law of excluded middle is also rejected in constructive mathematics, a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. Reformulations of calculus in a constructive framework are generally part of the subject of constructive analysis.[34]

Significance
While many of the ideas of calculus had been developed earlier in Greece, China, India, Iraq, Persia, and Japan, the use of calculus began in Europe, during the 17th century, when Newton and Leibniz built on the work of earlier mathematicians to introduce its basic principles.[11][25][42] The Hungarian polymath John von Neumann wrote of this work,

The calculus was the first achievement of modern mathematics and it is difficult to overestimate its importance. I think it defines more unequivocally than anything else the inception of modern mathematics, and the system of mathematical analysis, which is its logical development, still constitutes the greatest technical advance in exact thinking.[43]

Applications of differential calculus include computations involving velocity and acceleration, the slope of a curve, and optimization.[44]: 341–453  Applications of integral calculus include computations involving area, volume, arc length, center of mass, work, and pressure.[44]: 685–700  More advanced applications include power series and Fourier series.

Calculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involving division by zero or sums of infinitely many numbers. These questions arise in the study of motion and area. The ancient Greek philosopher Zeno of Elea gave several famous examples of such paradoxes. Calculus provides tools, especially the limit and the infinite series, that resolve the paradoxes.[45]

Principles
Limits and infinitesimals
Main articles: Limit of a function and Infinitesimal
Calculus is usually developed by working with very small quantities. Historically, the first method of doing so was by infinitesimals. These are objects which can be treated like real numbers but which are, in some sense, "infinitely small". For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positive real number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals. The symbols 
d
x
{\displaystyle dx} and 
d
y
{\displaystyle dy} were taken to be infinitesimal, and the derivative 
d
y
/
d
x
{\displaystyle dy/dx} was their ratio.[34]

The infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. In the late 19th century, infinitesimals were replaced within academia by the epsilon, delta approach to limits. Limits describe the behavior of a function at a certain input in terms of its values at nearby inputs. They capture small-scale behavior using the intrinsic structure of the real number system (as a metric space with the least-upper-bound property). In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by sequences of smaller and smaller numbers, and the infinitely small behavior of a function is found by taking the limiting behavior for these sequences. Limits were thought to provide a more rigorous foundation for calculus, and for this reason, they became the standard approach during the 20th century. However, the infinitesimal concept was revived in the 20th century with the introduction of non-standard analysis and smooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals.[34]

Differential calculus
Main article: Differential calculus

Tangent line at (x0, f(x0)). The derivative f′(x) of a curve at a point is the slope (rise over run) of the line tangent to that curve at that point.
Differential calculus is the study of the definition, properties, and applications of the derivative of a function. The process of finding the derivative is called differentiation. Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called the derivative function or just the derivative of the original function. In formal terms, the derivative is a linear operator which takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to produce another function. The function produced by differentiating the squaring function turns out to be the doubling function.[28]: 32 

In more explicit terms the "doubling function" may be denoted by g(x) = 2x and the "squaring function" by f(x) = x2. The "derivative" now takes the function f(x), defined by the expression "x2", as an input, that is all the information—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to output another function, the function g(x) = 2x, as will turn out.

In Lagrange's notation, the symbol for a derivative is an apostrophe-like mark called a prime. Thus, the derivative of a function called f is denoted by f′, pronounced "f prime" or "f dash". For instance, if f(x) = x2 is the squaring function, then f′(x) = 2x is its derivative (the doubling function g from above).

If the input of the function represents time, then the derivative represents change concerning time. For example, if f is a function that takes time as input and gives the position of a ball at that time as output, then the derivative of f is how the position is changing in time, that is, it is the velocity of the ball.[28]: 18–20 

If a function is linear (that is if the graph of the function is a straight line), then the function can be written as y = mx + b, where x is the independent v
{\displaystyle \lim _{h\to 0}{f(a+h)-f(a) \over {h}}.}
Geometrically, the derivative is the slope of the tangent line to the graph of f at a. The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the function f.[46]: 61–63 

Here is a particular example, the derivative of the squaring function at the input 3. Let f(x) = x2 be the squaring function.


The derivative f′(x) of a curve at a point is the slope of the line tangent to that curve at that point. This slope is determined by considering the limiting value of the slopes of the second lines. Here the function involved (drawn in red) is f(x) = x3 − x. The tangent line (in green) which passes through the point (−3/2, −15/8) has a slope of 23/4. The vertical and horizontal scales in this image are different.

{\displaystyle {\begin{aligned}f'(3)&=\lim _{h\to 0}{(3+h)^{2}-3^{2} \over {h}}\\&=\lim _{h\to 0}{9+6h+h^{2}-9 \over {h}}\\&=\lim _{h\to 0}{6h+h^{2} \over {h}}\\&=\lim _{h\to 0}(6+h)\\&=6\end{aligned}}}
The slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines the derivative function of the squaring function or just the derivative of the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.[46]: 63 

Leibniz notation
Main article: Leibniz's notation
A common notation, introduced by Leibniz, for the derivative in the example above is


{\displaystyle {\begin{aligned}g(t)&=t^{2}+2t+4\\{d \over dt}g(t)&=2t+2\end{aligned}}}
Even when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols like dx and dy as if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as the total derivative.

Integral calculus
Main article: Integral

Integration can be thought of as measuring the area under a curve, defined by f(x), between two points (here a and b).

A sequence of midpoint Riemann sums over a regular partition of an interval: the total area of the rectangles converges to the integral of the function.
Integral calculus is the study of the definitions, properties, and applications of two related concepts, the indefinite integral and the definite integral. The process of finding the value of an integral is called integration.[44]: 508  The indefinite integral, also known as the antiderivative, is the inverse operation to the derivative.[46]: 163–165  F is an indefinite integral of f when f is a derivative of F. (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.) The definite integral inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and the x-axis. The technical definition of the definite integral involves the limit of a sum of areas of rectangles, called a Riemann sum.[47]: 282 

A motivating example is the distance traveled in a given time.[46]: 153  If the speed is constant, only multiplication is needed:

{\displaystyle \mathrm {Distance} =\mathrm {Speed} \cdot \mathrm {Time} }
But if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (a Riemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.

When velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time. For example, traveling a steady 50 mph for 3 hours results in a total distance of 150 miles. Plotting the velocity as a function of time yields a rectangle with a height equal to the velocity and a width equal to the time elapsed. Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve.[44]: 535  This connection between the area under a curve and the distance traveled can be extended to any irregularly shaped region exhibiting a fluctuating velocity over a given period. If f(x) represents speed as it varies over time, the distance traveled between the times represented by a and b is the area of the region between f(x) and the x-axis, between x = a and x = b.

To approximate that area, an intuitive method would be to divide up the distance between a and b into several equal segments, the length of each segment represented by the symbol Δx. For each small segment, we can choose one value of the function f(x). Call that value h. Then the area of the rectangle with base Δx and height h gives the distance (time Δx multiplied by speed h) traveled in that segment. Associated with each segment is the average value of the function above it, f(x) = h. The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value for Δx will give more rectangles and in most cases a better approximation, but for an exact answer, we need to take a
 [스타연구소①] 김태리가 밝힌 '이름' 탄생 비화는?. OBS경인TV (in Korean). November 2, 2020. Archived from the original on March 29, 2024. Retrieved March 29, 2024.
 매니지먼트엠엠엠. Management MMM (in Korean). Archived from the original on May 27, 2022. Retrieved March 29, 2024.
 Koo, Jeong-mo (May 30, 2016). '아가씨' 김태리 "연기, 평생해도 재미있겠다 생각" ['Girl' Kim Tae-ri "I think acting will be fun for life"]. Naver (in Korean). Yonhap News Agency. Archived from the original on November 3, 2021. Retrieved May 15, 2021.
 Jin, Hyun-cheol (May 30, 2016). [인터뷰]김태리 "베드신보다 너털웃음이 더 어렵더라" [[Interview] Kim Tae-ri "It was harder to laugh more hard than bed scenes."]. MK (in Korean). Archived from the original on November 3, 2021. Retrieved May 15, 2021.
 Hyun, Hwa-young (June 18, 2016). [인터뷰] '아가씨' 김태리 "내 선택에 두려움 있었다" [[Interview]'Girl' Kim Tae-ri "I was afraid of my choice"]. Naver (in Korean). Segye Ilbo. Archived from the original on November 3, 2021. Retrieved May 15, 2021.
 Lee, Hae-ri (June 10, 2016). [인터뷰] 김태리, 박찬욱 감독과 한편 더? "시나리오 읽어봐야죠" [[Interview] Kim Tae-ri and Park Chan-wook and more? "I have to read the scenario"]. Sports Donga (in Korean). Archived from the original on April 10, 2023. Retrieved May 15, 2021.
 구, 정모. '아가씨' 김태리 "연기, 평생해도 재미있겠다 생각". Naver (in Korean). Archived from the original on November 3, 2021. Retrieved May 10, 2023.
 "KIM Tae-ri". Korean Film Council. Archived from the original on May 15, 2021. Retrieved May 15, 2021.
 서울독립영화제. Archived from the original on August 3, 2018. Retrieved November 19, 2020.
 "Moon young (2015)". Korean Film Biz Zone. Archived from the original on November 22, 2022. Retrieved November 19, 2020.
 박찬욱 '아가씨' 발탁 신예 김태리 누구? 1500대1 경쟁률 통과 (in Korean). Newsen. December 9, 2014. Archived from the original on May 31, 2022. Retrieved May 15, 2021.
 Kim, June (December 1, 2014). "KIM Min-hee and KIM Tae-ri Confirmed for FINGERSMITH". Korean Film Biz Zone. Archived from the original on October 26, 2020. Retrieved May 16, 2016.
 "Starlet in Spotlight as Debut Film Invited to Cannes". The Chosun Ilbo. May 7, 2016. Archived from the original on July 15, 2018. Retrieved May 16, 2016.
 Ahn, Sung-mi (May 2, 2016). "Will Kim Tae-ri become Park Chan-wook's new muse". The Korea Herald. Archived from the original on July 15, 2018. Retrieved May 16, 2016.
 "KIM Tae-ri Joins Cast of JANG Joon-hwan's 1987". Korean Film Biz Zone. February 3, 2017. Archived from the original on November 2, 2021. Retrieved April 29, 2018.
 "Kim Tae-ri Speaks out About Role in Democracy-Themed Movie". The Chosun Ilbo. January 6, 2018. Archived from the original on April 10, 2023. Retrieved April 29, 2018.
 "Kim Tae-ri looks back at a year full of work : In '1987,' she plays a student looking for truth". Korea JoongAng Daily. December 26, 2017. Archived from the original on November 22, 2022. Retrieved January 20, 2019.
 "'The Handmaiden' starlet to act in 'Little Forest'". Kpop Herald. September 19, 2016. Archived from the original on December 3, 2020. Retrieved July 7, 2017.
 "LITTLE FOREST's KIM Tae-ri". Korean Film Biz Zone. March 12, 2018. Archived from the original on November 22, 2022. Retrieved May 6, 2018.
 제54회 백상예술대상, TV·영화 각 부문별 수상 후보자 공개. JTBC (in Korean). April 6, 2018. Archived from the original on April 9, 2018. Retrieved April 6, 2018.
 청룡영화상 후보 발표, '1987' 최다·'공작'도 9개부문 후보. Newsen (in Korean). November 1, 2018. Archived from the original on February 22, 2020. Retrieved November 1, 2018.
 "Kim Tae-ri cast in latest Kim Eun-sook drama". Korea JoongAng Daily. July 7, 2017. Archived from the original on November 22, 2022. Retrieved January 20, 2019.
 Yoo, Chung-hee (April 4, 2019). 김서형·염정아·김혜자 등 '백상예술대상' TV부문 최종 후보 공개. Ten Asia (in Korean). Naver. Archived from the original on November 12, 2020. Retrieved April 4, 2019.
 "'Mr. Sunshine' gives new life to independence fighters". The Korea Times. October 1, 2018. Archived from the original on November 22, 2022. Retrieved January 20, 2019.
 "BLACKPINK, CL in Forbes 30 Under 30 Asia list". Manila Bulletin Entertainment. Archived from the original on April 28, 2019. Retrieved April 28, 2019.
 "Tae-ri Kim". Forbes. Archived from the original on November 28, 2020. Retrieved April 28, 2019.
 Choi, Ji-won (August 28, 2020). "Song Joong-ki's 'Space Sweepers' release shelved again". The Korea Herald. Archived from the original on August 29, 2020. Retrieved August 28, 2020.
 Lee, Gyu-lee (January 6, 2021). "Blockbuster 'Space Sweepers' to debut on Netflix Feb. 5". The Korea Times. Archived from the original on January 6, 2021. Retrieved January 6, 2021.
 ""CG가 압권" '승리호', 넷플릭스 월드와이드 1위...심상치 않은 후기". 톱스타뉴스 (in Korean). February 7, 2021. Archived from the original on February 14, 2021. Retrieved May 8, 2021.
 Lim, Jang-won (February 7, 2021). "'Space Sweepers' debuts at No. 1 on Netflix". The Korea Herald. Archived from the original on February 27, 2022. Retrieved February 7, 2021.
 Jung Yoo-jin (July 12, 2021). [단독]김태리‧전여빈, 제이와이드와 결별→신생 기획사 mmm과 새 출발 [[Exclusive] Kim Tae-ri and Jeon Yeo-bin break up with J-Wide → New start with the new agency MMM]. Sports TV News (in Korean). Naver. Archived from the original on July 12, 2021. Retrieved July 12, 2021.
 호불호 갈린 남주혁·김태리 이별 결말...'스물다섯 스물하나' 11.5% 자체 최고. Naver (in Korean). News1. Archived from the original on October 3, 2022. Retrieved April 4, 2022.
 Lee Ye-seul (May 6, 2022). 이준호x김태리, '백상' 인기상→ 최우수 연기상 나란히 수상 "더 좋은 배우 되겠다" [Jun-ho Lee x Tae-ri Kim, 'Baeksang' Popularity Award → Best Acting Award side by side "I will become a better actor"] (in Korean). OSEN [ko]. Archived from the original on May 6, 2022. Retrieved May 6, 2022 – via Naver.
 "Kim Tae-ri, Ryu Jun-yeol to Reunite in Sci-Fi Flick". The Chosun Ilbo. August 28, 2019. Archived from the original on August 28, 2019. Retrieved August 28, 2019.
 "Director CHOI Dong-hoon to Return with Simultaneously Filmed 2-Part Sci-Fi Film". Korean Film Biz Zone. September 16, 2019. Archived from the original on August 3, 2020. Retrieved May 12, 2020.
 Kim Nayeon (June 23, 2022). '외계+인' 김태리 "기계 체조→사격, 무술 준비 多" ['Alien + Human' Kim Tae-ri "Mechanical gymnastics → shooting, martial arts preparations"] (in Korean). Star News. Archived from the original on June 23, 2022. Retrieved June 23, 2022 – via Naver.
 Kim Sung-hyun (July 19, 2022). "Actress Kim Tae-ri Enjoys Challenges She Faces in Her Craft". The Chosun Ilbo. Archived from the original on July 20, 2022. Retrieved July 20, 2022 – via Naver.
 김태리→신예은 '정년이', 2024년 tvN 편성 확정...꿈의 무대가 온다. The Seoul Economic Daily [ko] (in Korean). November 17, 2023. Archived from the original on November 17, 2023. Retrieved August 9, 2024.
 배우 김태리, 오휘의 뮤즈가 되다. O hui Korea. September 25, 2017. Archived from the original on June 16, 2022.
 "How South Korean Actress Kim Tae Ri Became The New Face Of Flower By Kenzo". Her World. May 7, 2018. Archived from the original on April 10, 2023. Retrieved June 16, 2022.
 김태리, 티파니 앰버서더 존재감..단아+화사 매력. Naver. May 14, 2021. Archived from the original on April 10, 2023. Retrieved June 16, 2022.
 "Prada Welcomes Actress Kim Tae-Ri as New Brand Ambassador". Tatler Asia. October 7, 2021. Archived from the original on April 9, 2023. Retrieved June 16, 2022.
 김태리, 러블리한 봄의 요정..소녀 아우라[화보]. Herald POP. January 25, 2022. Archived from the original on April 10, 2023. Retrieved June 16, 2022.
 ""Moon Young", thanks to Kim Tae-ri". HanCinema. Dailian [ko]. January 3, 2017. Archived from the original on November 22, 2022. Retrieved October 27, 2017.
 Kim Na-ra (May 3, 2022). 류준열X김우빈→김태리X소지섭 '외계+인' 1부, 올여름 개봉 확정 [공식] [Ryu Jun-yeol X Kim Woo-bin → Kim Tae-ri X So Ji-seop 'Alien + Human' Part 1 confirmed for release this summer [Official]] (in Korean). My Daily. Archived from the original on May 3, 2022. Retrieved May 3, 2022 – via Naver.
 Lee, Ha-neul (October 26, 2023). 류준열·김태리·김우빈 '외계+인' 2부, 2024년 1월 개봉 확정 [Ryu Jun-yeol, Kim Tae-ri, Kim Woo-bin's 'Alien + Human' Part 2, confirmed to be released in January 2024] (in Korean). Ten Asia. Archived from the original on October 26, 2023. Retrieved October 26, 2023 – via Naver.
 Choi, Yoon-na (August 16, 2023). 김태리X홍경, 넷플릭스 첫 韓 애니 영화 '이 별에 필요한' 목소리 캐스팅 [공식] [Kim Tae-ri X Hong-kyung, Netflix's first Korean animation film 'Necessary for this star' voice casting [Official]] (in Korean). Sports Donga. Archived from the original on August 16, 2023. Retrieved August 16, 2023 – via Naver.
 ""The Handmaiden" Park Chan-wook, Ha Jung-woo and Kim Tae-ri finish their parts in "Entourage"". HanCinema. Wow TV. August 8, 2016. Archived from the original on November 3, 2021. Retrieved April 29, 2018.
 Lee Min-ji (September 7, 2021). 스물다섯 스물하나' 김태리-남주혁-보나-최현욱-이주명 출연확정(공식) [Twenty-five Twenty One' Kim Tae-ri, Nam Joo-hyuk, Bona, Choi Hyun-wook, and Lee Joo-myung confirmed to appear (official)]. Newsen (in Korean). Archived from the original on October 10, 2021. Retrieved September 7, 2021 – via Naver.
 Hwang Hye-jin (September 21, 2022). 김태리X오정세X홍경, 김은희 신작 '악귀' 출연 확정 [공식] [Kim Tae-ri X Oh Jeong-se X Hong Kyung, Kim Eun-hee confirmed to appear in the new 'Devil' [Official]] (in Korean). Newsen. Archived from the original on September 21, 2022. Retrieved September 21, 2022 – via Naver.
 Tae Yu-na (August 10, 2023). [공식] 김태리, 판소리 천재 된다...'더 글로리' 신예은과 '정년이'로 호흡 [[Official] Kim Tae-ri becomes a pansori genius... 'The Glory' Shin Ye-eun and 'Jung Nyeon' breathing together] (in Korean). Ten Asia. Archived from the original on August 10, 2023. Retrieved August 10, 2023 – via Naver.
 당신의 지금 그 사랑, 진짜일까요. Munhwa Ilbo (in Korean). Archived from the original on May 13, 2023. Retrieved May 10, 2023.
 Jeong, Sang-young (September 3, 2013). 가수 꿈꾸는 자폐아...탄광촌 화가들...꿈을 잃은 당신 초대합니다. The Hankyoreh (in Korean). Archived from the original on May 11, 2023. Retrieved May 10, 2023.
 김태리 대학생 시절 연극..조연출까지. 100세시대의 동반자 브릿지경제 (in Korean). January 4, 2018. Archived from the original on May 10, 2023. Retrieved May 10, 2023.
 Lee, Yoon-jeong (October 16, 2013). '멍든 꽃=멍든 사회'...연극 '팬지' 내달 공연. Edaily [ko] (in Korean). Retrieved May 10, 2023.
 Peak chart positions on Gaon Digital Chart:
"With". Gaon Music Chart (in Korean). Archived from the original on March 31, 2022. Retrieved March 31, 2022.
 Peak chart positions on Billboard K-Pop Hot 100:
"With". Billboard Korea (in Korean). April 2, 2022. Archived from the original on March 29, 2022. Retrieved March 29, 2022.
 세기말의 노래 Segimalui Norae (The Song at the End of the Century) [Segimalui Norae (The Song at the End of the Century)] (in Korean). August 28, 2016. Archived from the original on May 31, 2022. Retrieved July 19, 2021 – via YouTube.
 가리워진 길 – 김태리,강동원(1987 OST) [The Hidden Road – Kim Tae-ri, Kang Dong-won (1987 OST)] (in Korean). January 21, 2018. Archived from the original on May 31, 2022. Retrieved July 19, 2021 – via YouTube.
 (Original Soundtrack) by Kim Tae Seong, January 15, 2018, archived from the original on July 6, 2023, retrieved December 10, 2018
 김태리,강동원 – 가리워진 길 (연희) [1987 OST] [Kim Tae-ri, Kang Dong-won – The Hidden Road (Yeonhee) [1987 OST]] (in Korean). January 16, 2018. Archived from the original on July 19, 2021. Retrieved July 19, 2021 – via YouTube.
 Park Seo-yeon (March 11, 2022). 김태리→남주혁, '스물다섯 스물하나' OST 직접 부른다..13일 'With' 발매[공식] [Tae-ri Kim → Joo-hyuk Nam, sang 'Twenty-Five Twenty One' OST himself.. 13th 'With' Released [Official]] (in Korean). Herald POP. Archived from the original on March 11, 2022. Retrieved March 11, 2022 – via Naver.
 양세종X장기용X김태리X원진아, 남녀신인상 공동수상 영예[2018 APAN]. OSEN [ko] (in Korean). October 13, 2018. Archived from the original on October 14, 2018. Retrieved October 14, 2018.
 Kim Jae-won (August 31, 2022). "2022 에이판 스타 어워즈, 각 부문 후보자(작) 공개" [2022 APAN Star Awards, nominees (works) in each category revealed] (in Korean). Sports World. Archived from the original on August 31, 2022. Retrieved August 31, 2022 – via Naver.
 Hwang, Hyo-yi (September 6, 2022). 강태오-박은빈'·'손석구-김지원'·'이준호-이세영' 베스트커플상은? [Best Couple Award for 'Kang Tae-oh – Park Eun-bin', 'Son Seok-gu-Kim Ji-won', 'Lee Jun-ho-Lee Se-young'?] (in Korean). Sports Kyunghyang. Archived from the original on September 6, 2022. Retrieved September 6, 2022 – via Naver.
 "Kim Hee Sun & EXO win Grand Prize Awards at the 2017 Asia Artist Awards". KBS World. November 16, 2017. Archived from the original on July 15, 2018. Retrieved May 6, 2018.
 "Asian Film Awards: South Korean Erotic Thriller 'The Handmaiden' Wins Big". The Hollywood Reporter. March 21, 2017. Archived from the original on June 24, 2018. Retrieved April 29, 2018.
 Ahn Byung-gil (April 11, 2022). 58회 백상예술대상 후보 공개...영광의 주인공은? [58th Baeksang Arts Awards nominations revealed... Who is the hero of glory?] (in Korean). Sports Kyunghyang. Archived from the original on May 10, 2022. Retrieved April 11, 2022 – via Naver.
 Jeong Hee-yeon (May 6, 2022). 불철주야 전쟁 끝에" 이준호-김태리 틱톡 인기상 (백상예술대상) ["At the end of the day and night war" Lee Jun-ho and Kim Tae-ri TikTok Popularity Award (Baeksang Arts Awards)] (in Korean). Sports Donga. Archived from the original on November 11, 2020. Retrieved May 6, 2022 – via Naver.
 "Inside Men wins best film at Blue Dragon Awards". Yonhap News Agency. November 25, 2016. Archived from the original on September 26, 2018. Retrieved September 26, 2018.
 Lee, Seung-gil (September 1, 2022). 유재석·임영웅·아이브 등, '2022 올해의 브랜드 대상' 수상 [공식] [Jae-seok Yoo, Young-woong Lim, Ive, etc., won the '2022 Brand of the Year Grand Prize' [Official]] (in Korean). My Daily. Archived from the original on November 12, 2020. Retrieved September 1, 2022 – via Naver.
 Kil, Sonia (October 7, 2016). "Busan: Bu-il Awards Provide Counterpoint to Festival". Variety. Archived from the original on December 22, 2016. Retrieved October 7, 2016.
 김희애·나문희·김태리 등 부일영화상 女주연상 격돌. Star News (in Korean). August 25, 2018. Archived from the original on April 6, 2020. Retrieved September 26, 2018.
 "2022 부일영화상 on Instagram: "⠀ 여러분들의 뜨거운 관심 속 투표를 종료했던 올해의 스타상! 여자 부문 최종 집계 상위 득표자 TOP10을 공개합니다!..🥁 * 무순 임윤아 배두나 이주영 이지은 전도연 천우희 김태리 고윤정 전혜진 탕웨이 투표 마감날까지 정말 많은 분들이 참여를 해주셨습니다❤ 과연 여러분들이 응원하는 배우가 '올해의 스타상'을 받을 수 있을지?! 결과가 궁금하다면~! ✨10월 6일 목요일 📡 부산 MBC 17시 (시상식) 📡 네이버 NOW 16시 (레드카펫 / 시상식) #2022부일영화상 #부일영화상 #올해의스타상 #임윤아 #배두나 #이주영 #이지은 #iu #전도연 #천우희 #김태리 #고윤정 #전혜진 #탕웨이 #인기스타상"". Instagram. Archived from the original on October 9, 2022. Retrieved October 9, 2022.
 제17회 부산영화평론가협회상, '비밀은 없다'가 선정. Newsis (in Korean). November 28, 2016. Archived from the original on August 13, 2018. Retrieved September 26, 2018.
 제23회 춘사영화제 5월18일 개최..홍상수·김민희 참석하나. Newsen (in Korean). May 3, 2018. Archived from the original on April 2, 2019. Retrieved May 3, 2018.
 "2019 춘사영화제, 7월 18일 개최..트로피 주인공 누구?". K Starnews (in Korean). July 3, 2019. Archived from the original on November 17, 2020. Retrieved July 3, 2019.
 <아가씨> 김태리. Cine21 (in Korean). December 19, 2016. Archived from the original on October 2, 2018. Retrieved November 28, 2018.
 Lee, Ji-hae (August 9, 2016). "Winners of annual Korean film awards announced". Kpop Herald. Archived from the original on October 12, 2016. Retrieved September 11, 2016.
 "KIM Tae-ri and LEE Sung-min Crowned Best Actress and Actor at Director's Cut". Korean Film Biz Zone. December 24, 2018. Archived from the original on December 30, 2018. Retrieved December 26, 2018.
 제55회 대종상, 각 부문 후보 공개...'공작' 12개 최다부문 노미네이트. Sports Seoul (in Korean). September 21, 2018. Archived from the original on September 21, 2018. Retrieved September 21, 2018.
 "The Shortlist of the 36th Huading Awards Global Unit Announced, 10 Films Including "All Quiet on the Western Front" Shortlisted for Best Picture". Business Wire. March 22, 2023. Archived from the original on April 6, 2023. Retrieved March 23, 2023.
 Kim Ji-woo (December 21, 2022). 뜨거웠다' 박해일, 박은빈 그리고 손석구 ['It Was Hot' Park Hae-il, Park Eun-bin and Son Seok-gu] (in Korean). Sports Kyunghyang. Archived from the original on December 22, 2022. Retrieved December 21, 2022 – via Naver.
 "'The Wailing' named best film by Korean film reporters". Yonhap News Agency. January 18, 2017. Archived from the original on January 31, 2017. Retrieved January 18, 2017.
 Kim, Ki-hoon (October 13, 2022). 배우 김태리, '광고주가 뽑은 모델상'에 선정 [Actor Kim Tae-ri, selected as the 'Advertiser's Model Award]. Yonhap News Agency (in Korean). Archived from the original on October 13, 2022. Retrieved October 13, 2022 – via Naver.
 '악귀' 김태리, 제51회 한국방송대상 최우수연기자상 수상. KBS 스타연예 (in Korean). September 3, 2024. Archived from the original on September 8, 2024. Retrieved September 8, 2024.
 "2018 코리아드라마어워즈(KDA) 수상후보 및 작품 공개...10월 2일 경남 진주 경남문화예술회관 개최". 톱스타뉴스 (in Korean). September 28, 2018. Archived from the original on July 6, 2023. Retrieved October 9, 2022.
 지, 성호. '코리아드라마페스티벌' 3년 만에 진주서 열린다...8일 개막. Naver News (in Korean). Archived from the original on October 9, 2022. Retrieved October 9, 2022.
 전국 청소년이 뽑은 인기 영화인은?. Naver (in Korean). News1. October 7, 2017. Archived from the original on October 12, 2013. Retrieved April 29, 2018.
 차태현, 김태리 청소년이 뽑은 인기스타. Gg Ilbo (in Korean). November 1, 2018. Archived from the original on November 7, 2018. Retrieved November 28, 2018.
 Jang, Jin-ri (December 30, 2023). '모범택시2' 이제훈-'악귀' 김태리, 공동 대상 "우열 못 가려"[종합](SBS 연기대상) [Model Taxi 2's" Lee Ji-hoon and "Demon" Kim Tae-ri shared the Grand Prize for "Unequal" [Overall] (SBS Acting Awards)]. SpoTV News (in Korean). Archived from the original on December 30, 2023. Retrieved December 30, 2023 – via Naver.
 Kim, Ye-seol (December 30, 2023). 이제훈·김태리, "우열 가릴 수 없어" 'SBS 연기대상' 쪼개진 수상에 가위바위보 진풍경? [종합] [Lee Je-hoon, Kim Tae-ri, "I can't tell the difference" 'SBS Acting Awards' split awards? [Synthesis].]. OSEN [ko] (in Korean). Archived from the original on December 31, 2023. Retrieved December 31, 2023 – via Naver.
 '제2회 더 서울어워즈' 10월27일 개최, 드라마-영화 각 부문별 후보공개. iMBC (in Korean). September 28, 2018. Archived from the original on April 4, 2019. Retrieved September 28, 2018.
 '신필름예술영화제' 신성일 공로상-김태리 최은희 배우상 받는다. Newsen (in Korean). August 30, 2018. Archived from the original on September 26, 2018. Retrieved September 26, 2018.
 "영화·영상 전공 대학생들의 축제"···제13회 대한민국 대학영화제 열렸다. Insight (in Korean). November 24, 2018. Archived from the original on December 27, 2020. Retrieved December 16, 2018.
 "Women in Film Korea Festival Held on December 7". Korean Film Biz Zone. December 12, 2016. Archived from the original on November 30, 2020. Retrieved July 8, 2017.
 Hicap, Jonathan (October 18, 2018). "BTS, Red Velvet win at Korean Popular Culture and Arts Awards". Manila Bulletin. Archived from the original on October 18, 2018. Retrieved June 18, 2021.
 Yeo, Yer-im (October 25, 2018). "BTS gets award upon their return home". Yonhap News Agency. Archived from the original on November 7, 2018. Retrieved June 18, 2021 – via Korea JoongAng Daily.
 Lee, Sang-won (October 25, 2016). "Korean Popular Culture and Arts Awards announces winners". The Korea Herald. Archived from the original on September 29, 2018. Retrieved June 18, 2021.
 [대중문화예술상] 레드벨벳-박나래-김태리-국가스텐 등, 문화체육관광부 장관표창. TopStarNews (in Korean). October 24, 2018. Archived from the original on November 16, 2018. Retrieved November 16, 2018.
 파워리더 2030 ENTERTAINMENT] 김태리 배우 외 1그룹, 3인. Naver (in Korean). Forbes Korea. January 23, 2018. Archived from the original on February 1, 2018. Retrieved January 23, 2018.
 Kim, Min-soo (April 23, 2019). 파워 셀럽 40인은 누구? [Who are the 40 power celebrities?]. JoongAng Ilbo (in Korean). Archived from the original on May 2, 2019. Retrieved May 21, 2021 – via Joins.
 "2023 파워 셀러브리티 40 | 세계가 주목하는 K-셀럽 파워". 중앙시사매거진. April 23, 2023. Archived from the original on June 4, 2023. Retrieved May 3, 2023.
 한국갤럽조사연구소. www.gallup.co.kr. Archived from the original on December 14, 2023. Retrieved December 14, 2023.